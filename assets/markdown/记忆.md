# 1. 短期记忆
LLM的核心架构在推理时，每一轮都是独立的计算过程。因此如果不把所有对话历史都带上的话，LLM就只能针对当前问题就会回答，无法感知历史对话。

在AI中，短期记忆通常指“对话上下文”，包括：
- prompt
- 对话历史
- 前序大模型推理结果
- 前序工具执行结果

短期记忆有两种存储方式：
- 内存：进程重启后丢失
- 数据库：进程重启后可以根据thread_id找回
## 1.1. 局限性
**LLM上下文窗口大小有限**：容易造成上下文污染，大模型找不到重点。
- 滑动窗口：仅保留最近N轮对话
- 总结压缩：先让LLM将之前的对话总结成一段简短的摘要
# 2. 对比

| **特性**   | **短期记忆 (Checkpoint/Postgres)**       | **长期记忆 (Long-term Memory)**  |
| -------- | ------------------------------------ | ---------------------------- |
| **存储内容** | 逐字逐句的对话记录、节点状态                       | 事实、用户偏好、总结后的知识               |
| **技术实现** | **内存**或**PostgreSQL (Checkpointer)** | **向量数据库 (Vector DB)** 或 关系型表 |
| **检索方式** | 按 `thread_id` 全量加载                   | 按语义相关性检索 (RAG)               |
| **典型例子** | “他刚才说他叫小明”                           | “用户在三个月前提到过他花生过敏”            |
| **容量限制** | 受模型上下文窗口限制                           | 理论上无限                        |
